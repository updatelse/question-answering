def _attention(self, input_v, x_q_vec):
        batch_size = input_v.size(0)
        width = input_v.size(2)
        height = input_v.size(3)

                                # Process visual before fusion
                               ####### x_v = input_v.view(batch_size*width*height, dim_features)
        x_v = input_v
        x_v = F.dropout(x_v,
                        p=self.opt['attention']['dropout_v'],
                        training=self.training)
        x_v = self.conv_v_att(x_v)
        if 'activation_v' in self.opt['attention']:
            x_v = getattr(F, self.opt['attention']['activation_v'])(x_v)
        x_v = x_v.view(batch_size,
                       self.opt['attention']['dim_v'],
                       width * height)                    # （batchsize=512, 2048，长*宽）
        x_v = x_v.transpose(1,2)                          # (batchsize=512, 长*宽， 2048)     #进行了转置

        # Process question before fusion
        x_q = F.dropout(x_q_vec, p=self.opt['attention']['dropout_q'],
                           training=self.training)
        x_q = self.linear_q_att(x_q) 
        if 'activation_q' in self.opt['attention']:
            x_q = getattr(F, self.opt['attention']['activation_q'])(x_q)
        x_q = x_q.view(batch_size,
                       1,
                       self.opt['attention']['dim_q'])  # x_q = (batchsize=512, 1， 2400)
        x_q = x_q.expand(batch_size,
                         width * height,
                         self.opt['attention']['dim_q'])  # x_q = (batchsize=512, 长*宽， 2400)   为了与x_v匹配

        # 第一次融合
        # 调用fusion.py中的 MutanFusion2d（）
        x_att = self._fusion_att(x_v, x_q)

        if 'activation_mm' in self.opt['attention']:
            x_att = getattr(F, self.opt['attention']['activation_mm'])(x_att)   #第一次融合能得到x_att

        # 处理注意向量
        x_att = F.dropout(x_att,
                          p=self.opt['attention']['dropout_mm'],
                          training=self.training)
        # can be optim to avoid two views and transposes
        x_att = x_att.view(batch_size,
                           width,
                           height,
                           self.opt['attention']['dim_mm'])           # （batchsize=512, 14, 14, 510）
        x_att = x_att.transpose(2,3).transpose(1,2)                   # (batchsize=512, 510, 14, 14)

        # 输入 [batchsize=512, 通道=510， height=14， width=14]
        # 卷积 [通道=510， 输出深度2， 过滤器宽1， 过滤器宽1]
        # 结果 [batchsize, 输出深度2， 14， 14]
        x_att = self.conv_att(x_att)
        x_att = x_att.view(batch_size,
                           self.opt['attention']['nb_glimpses'],
                           width * height)                        # (batchsize, 2, 14*14)
        list_att_split = torch.split(x_att, 1, dim=1)  # (batchsize, 1, 14*14)
        list_att = []
        for x_att in list_att_split:
            x_att = x_att.contiguous()
            x_att = x_att.view(batch_size, width*height)          #(batchsize,14*14)
            x_att = F.softmax(x_att)
            list_att.append(x_att)

        self.list_att = [x_att.data for x_att in list_att]

        # Apply attention vectors to input_v    把注意力向量应用到input_v中
        x_v = input_v.view(batch_size, self.opt['dim_v'], width * height)        #(batchsize,2048,14*14)    
        x_v = x_v.transpose(1,2)        #(batchsize=512,14*14,2048)      #转置

        list_v_att = []
        for i, x_att in enumerate(list_att):      #循环枚举进行拼接
            x_att = x_att.view(batch_size,        #通过view函数将(batchsize、宽*高、1)三张量拼接在一起    共2048个
                               width * height,
                               1)
            x_att = x_att.expand(batch_size,
                                 width * height,
                                 self.opt['dim_v'])
            #     实现  x_att(batchsize, 14*14, 2048) 
            #     实现  x_v(batchsize, 14*14, 2048)
            
            x_v_att = torch.mul(x_att, x_v)   # 使用mul对x_att和x_v进行向量对应位相乘
            x_v_att = x_v_att.sum(1)          #(batchsize，1，dim_v)
            x_v_att = x_v_att.view(batch_size, self.opt['dim_v'])            # （batchsize，2048）
            list_v_att.append(x_v_att)

        return list_v_att    #返回了图像注意力列表  第72行
